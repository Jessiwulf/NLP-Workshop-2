{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dba5c98",
   "metadata": {},
   "source": [
    "# Workshop 2 - Data Prep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7816153",
   "metadata": {},
   "source": [
    "Submitted by Jirapat Sereerat 662115004\n",
    "\n",
    "Use the IMDB Dataset of 50K Movie Reviews (train and test)\n",
    "- Step 1: Text cleaning\n",
    "    - Remove special chars, numbers, and extra spaces\n",
    "- Step 2. Tokenization\n",
    "    - Split into sentences and words\n",
    "- Step 3. Lowercasing and Stop word removal\n",
    "    - Covert text to lowercase\n",
    "- Step 4 Emoticons, Stemming and Lemmatization\n",
    "  - Final: \n",
    "    1. Check readability score Flesch-Kincaid (report in class)\n",
    "    2. Regex Explanation Provide a manual \"translation\" of the most complex\n",
    "\n",
    "*Regular Expression used in Step 1*\n",
    "\n",
    "***Manual function is just experiment from the slide "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6578f",
   "metadata": {},
   "source": [
    "## Setup and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73cf14e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# !pip install textstat\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import textstat\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "df = pd.read_csv('IMDB_dataset/IMDB Dataset.csv', encoding='latin1')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be12a3",
   "metadata": {},
   "source": [
    "## Step 1 - Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a37a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  One of the other reviewers has mentioned that ...  \n",
      "1  A wonderful little production. The filming tec...  \n",
      "2  I thought this was a wonderful way to spend ti...  \n",
      "3  Basically there's a family where a little boy ...  \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  \n"
     ]
    }
   ],
   "source": [
    "# Emoticon Dictionary\n",
    "emoticon_map = {\n",
    "    \":)\": \"happy\", \":(\": \"sad\", \":D\": \"laugh\", \";)\": \"wink\", \":-(\": \"sad\", \n",
    "    \"<3\": \"love\", \":-P\": \"tongue\"\n",
    "}\n",
    "\n",
    "# Manual Emoticon Function (Simple Replace)\n",
    "# def manual_emoticon_handling(text):\n",
    "#     for emo, word in emoticon_map.items():\n",
    "#         text = text.replace(emo, word)\n",
    "#     return text\n",
    "\n",
    "# re Emoticon Function (Regex / Import re) - ACTIVE \n",
    "def re_emoticon_handling(text):\n",
    "    for emo, word in emoticon_map.items():\n",
    "        pattern = re.escape(emo)\n",
    "        text = re.sub(pattern, \" \" + word + \" \", text)\n",
    "    return text\n",
    "\n",
    "# CLEANING\n",
    "def clean_text(text):\n",
    "    # Handle Emoticons\n",
    "    text = re_emoticon_handling(text) \n",
    "    # text = manual_emoticon_handling(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers (Regex) ***can use to remove emoticons\n",
    "    text = re.sub(r'(?<=[.,!?])(?=[a-zA-Z])', ' ', text)\n",
    "\n",
    "    # Remove Special Characters BUT KEEP PUNCTUATION\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "print(df[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7d495",
   "metadata": {},
   "source": [
    "## Step 2 & 3 - Tokenization, Lowercasing, and Stop Words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3d19c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [one, reviewers, mentioned, watching, 1, oz, e...\n",
      "1    [wonderful, little, production., filming, tech...\n",
      "2    [thought, wonderful, way, spend, time, hot, su...\n",
      "3    [basically, there's, family, little, boy, jake...\n",
      "4    [petter, mattei's, \"love, time, money\", visual...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_and_prep(text):\n",
    "    # Lowercasing \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove Stop words\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "df['tokens'] = df['cleaned_review'].apply(tokenize_and_prep)\n",
    "\n",
    "print(df['tokens'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2208b3",
   "metadata": {},
   "source": [
    "## Step 4 - Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e6a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens  \\\n",
      "0  [one, reviewers, mentioned, watching, 1, oz, e...   \n",
      "1  [wonderful, little, production., filming, tech...   \n",
      "2  [thought, wonderful, way, spend, time, hot, su...   \n",
      "3  [basically, there's, family, little, boy, jake...   \n",
      "4  [petter, mattei's, \"love, time, money\", visual...   \n",
      "\n",
      "                                             stemmed  \\\n",
      "0  [one, review, mention, watch, 1, oz, episod, h...   \n",
      "1  [wonder, littl, production., film, techniqu, u...   \n",
      "2  [thought, wonder, way, spend, time, hot, summe...   \n",
      "3  [basic, there', famili, littl, boy, jake, thin...   \n",
      "4  [petter, mattei', \"love, time, money\", visual,...   \n",
      "\n",
      "                                          lemmatized  \n",
      "0  [one, reviewer, mentioned, watching, 1, oz, ep...  \n",
      "1  [wonderful, little, production., filming, tech...  \n",
      "2  [thought, wonderful, way, spend, time, hot, su...  \n",
      "3  [basically, there's, family, little, boy, jake...  \n",
      "4  [petter, mattei's, \"love, time, money\", visual...  \n"
     ]
    }
   ],
   "source": [
    "# --- STEMMING ---\n",
    "# Manual Stemming (Suffix Stripping)\n",
    "# def manual_stemmer(word):\n",
    "#     if word.endswith('ing'): return word[:-3]\n",
    "#     if word.endswith('ed'): return word[:-2]\n",
    "#     return word\n",
    "\n",
    "# Import Stemming (NLTK PorterStemmer) - ACTIVE\n",
    "ps = PorterStemmer()\n",
    "def import_stemmer(word):\n",
    "    return ps.stem(word)\n",
    "\n",
    "# --- LEMMATIZATION ---\n",
    "# Manual Lemmatization (Dictionary) \n",
    "# lemma_dict = {\"better\": \"good\", \"ran\": \"run\", \"movies\": \"movie\"}\n",
    "# def manual_lemmatizer(word):\n",
    "#     return lemma_dict.get(word, word)\n",
    "\n",
    "# Import Lemmatization (NLTK WordNet) - ACTIVE\n",
    "wnl = WordNetLemmatizer()\n",
    "def import_lemmatizer(word):\n",
    "    return wnl.lemmatize(word)\n",
    "\n",
    "def step_4_pipeline(tokens):\n",
    "    stemmed_tokens = [import_stemmer(w) for w in tokens]\n",
    "    lemmatized_tokens = [import_lemmatizer(w) for w in tokens]\n",
    "    return stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "df[['stemmed', 'lemmatized']] = df['tokens'].apply(\n",
    "    lambda x: pd.Series(step_4_pipeline(x))\n",
    ")\n",
    "\n",
    "print(df[['tokens', 'stemmed', 'lemmatized']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73478cbc",
   "metadata": {},
   "source": [
    "## Final - Readability & Regex Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a4025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- READABILITY COMPARISON (Flesch Reading Ease) ---\n",
      "Average Score (Original Text): 64.14\n",
      "Average Score (Cleaned Text):  64.44\n",
      "------------------------------\n",
      "\n",
      "File 'IMDB_Processed_Comparison.csv' has been generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Flesch Reading Ease\n",
    "def get_readability(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "# Original\n",
    "df['readability_score_original'] = df['review'].apply(get_readability)\n",
    "\n",
    "# Cleaned\n",
    "df['readability_score_cleaned'] = df['cleaned_review'].apply(get_readability)\n",
    "\n",
    "avg_original = df['readability_score_original'].mean()\n",
    "avg_cleaned = df['readability_score_cleaned'].mean()\n",
    "\n",
    "print(\"--- READABILITY COMPARISON (Flesch Reading Ease) ---\")\n",
    "print(f\"Average Score (Original Text): {avg_original:.2f}\")\n",
    "print(f\"Average Score (Cleaned Text):  {avg_cleaned:.2f}\")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "# --- Generate Processed File ---\n",
    "output_columns = [\n",
    "    'review', \n",
    "    'cleaned_review', \n",
    "    'tokens', \n",
    "    'stemmed', \n",
    "    'lemmatized', \n",
    "    'readability_score_original', \n",
    "    'readability_score_cleaned'\n",
    "]\n",
    "\n",
    "df_final = df[output_columns]\n",
    "\n",
    "# Save to CSV\n",
    "df_final.to_csv('IMDB_Processed_Comparison.csv', index=False)\n",
    "print(\"\\nFile 'IMDB_Processed_Comparison.csv' has been generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2e779",
   "metadata": {},
   "source": [
    "### Regular Expressions (Regex) Explanation\n",
    "\n",
    "#### 1. HTML Tag Removal\n",
    "**Code:** `re.sub(r'<[^>]*>', ' ', text)`\n",
    "\n",
    "**Translation:** \"Find any text that starts with `<` and ends with `>`, and replace it with a space.\"\n",
    "\n",
    "| Symbol | Function |\n",
    "| :--- | :--- |\n",
    "| **`<`** | Matches the starting angle bracket. |\n",
    "| **`[^>]*`** | Matches any character that is **NOT** a closing bracket (`>`), repeated zero or more times. |\n",
    "| **`>`** | Matches the closing angle bracket. |\n",
    "\n",
    "* **Before:** `\"This movie<br />was terrible.\"`\n",
    "* **After:** `\"This movie was terrible.\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Grammar Fixer (The \"Space Inserter\")\n",
    "**Code:** `re.sub(r'(?<=[.,!?])(?=[a-zA-Z])', ' ', text)`\n",
    "\n",
    "**Translation:** \"Find the **position** right after a punctuation mark and right before a letter, and insert a space there.\"\n",
    "\n",
    "This uses **Lookarounds** (Zero-width assertions). It doesn't consume characters; it only checks boundaries.\n",
    "\n",
    "| Symbol | Name | Function |\n",
    "| :--- | :--- | :--- |\n",
    "| **`(?<=...)`** | **Positive Lookbehind** | Checks if the previous character matches the set `[.,!?]`. |\n",
    "| **`(?=...)`** | **Positive Lookahead** | Checks if the next character matches a letter `[a-zA-Z]`. |\n",
    "\n",
    "* **Why?** Many reviews have typos like `\"movie.It\"` which confuse NLP models.\n",
    "* **Before:** `\"I loved the film.It was great.\"`\n",
    "* **After:** `\"I loved the film. It was great.\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Smart Special Character Cleaning\n",
    "**Code:** `re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"]', '', text)`\n",
    "\n",
    "**Translation:** \"Delete any character that is **NOT** a letter, number, space, or standard punctuation mark.\"\n",
    "\n",
    "Unlike standard cleaning (which deletes everything), this \"Whitelist\" approach preserves sentence structure.\n",
    "\n",
    "| Symbol | Function |\n",
    "| :--- | :--- |\n",
    "| **`^`** | When inside `[...]`, it means **NOT**. Matches everything *except* what follows. |\n",
    "| **`a-zA-Z`** | Matches all English letters. |\n",
    "| **`0-9`** | Matches numbers. |\n",
    "| **`\\s`** | Matches whitespace (spaces, tabs). |\n",
    "| **`.,!?\\'\"`** | **The Whitelist:** Explicitly keeps dots, commas, exclamation marks, question marks, and quotes. |\n",
    "\n",
    "* **Before:** `\"Wow!!! It cost $100 & was 100% fun :)\"`\n",
    "* **After:** `\"Wow!!! It cost 100  was 100 fun \"` (Note: `&`, `$`, `%` are removed; `!` is kept).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Extra Space Removal\n",
    "**Code:** `re.sub(r'\\s+', ' ', text)`\n",
    "\n",
    "**Translation:** \"Find any sequence of one or more spaces and replace them with a single space.\"\n",
    "\n",
    "| Symbol | Function |\n",
    "| :--- | :--- |\n",
    "| **`\\s`** | Matches whitespace (space, tab, newline). |\n",
    "| **`+`** | Quantifier meaning \"one or more times\". |\n",
    "\n",
    "* **Before:** `\"Movie    was  good.\"`\n",
    "* **After:** `\"Movie was good.\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Sentence Splitting (For Manual Score)\n",
    "**Code:** `re.split(r'[.!?]+', text)`\n",
    "\n",
    "**Translation:** \"Split the text into a list whenever you see one or more periods, exclamation marks, or question marks.\"\n",
    "\n",
    "| Symbol | Function |\n",
    "| :--- | :--- |\n",
    "| **`[...]`** | Character set matching dot, exclamation, or question mark. |\n",
    "| **`+`** | Matches one or more (handles multiple marks like `..` or `?!`). |\n",
    "\n",
    "* **Input:** `\"Really?! I didn't know.\"`\n",
    "* **Output List:** `[\"Really\", \" I didn't know\", \"\"]`\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Emoticon Escaping\n",
    "**Code:** `re.escape(emo)`\n",
    "\n",
    "**Translation:** \"Automatically add backslashes `\\` before special characters in a string so Regex treats them as text, not commands.\"\n",
    "\n",
    "* **Input:** `\":)\"`\n",
    "* **Output:** `\"\\:\\)\"` (Tells regex to look for a literal colon and parenthesis, not a group).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
